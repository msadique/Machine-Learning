{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "with open('cleaned_data.txt') as f:\n",
    "    for line in f:\n",
    "        inner_list = [int(elt.strip()) for elt in line.split(',')]\n",
    "        list_of_lists.append(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "for i in list_of_lists:\n",
    "    a = i[:len(i)-1]\n",
    "    data.append(a)\n",
    "    #print(i[len(i)-1])\n",
    "    target.append(i[len(i)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(data,target,train_size=.67, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C =  5 : 0.65246(+/- 0.03050)\n",
      "Accuracy with C =  2 : 0.65048(+/- 0.02919)\n",
      "Accuracy with C =  1 : 0.65033(+/- 0.03495)\n",
      "Accuracy with C =  0.1 : 0.64713(+/- 0.03642)\n",
      "Accuracy with C =  0.01 : 0.63298(+/- 0.03028)\n",
      "Accuracy with C =  0.001 : 0.59673(+/- 0.03964)\n",
      "Accuracy with C =  0.002 : 0.60633(+/- 0.03264)\n",
      "Accuracy with C =  0.0001 : 0.56568(+/- 0.02823)\n",
      "Accuracy with C =  1e-05 : 0.51924(+/- 0.03680)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Calculating accuracy of LOGISTIC REGRESSION using CROSS_VAL_SCORE\n",
    "CR= [8,4,2,1,0.1,0.01,0.001,0.002,0.0001,0.00001]\n",
    "for i in CR: \n",
    "    clf = LogisticRegression(C = i, max_iter = 1000, random_state=0)\n",
    "    scoresLR = cross_val_score(clf,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "    print(\"Accuracy with CR = \",i,\": %0.5f(+/- %0.5f)\" %(scoresLR.mean(), scoresLR.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Decision Tree: 0.76515 (+/- 0.03727)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "scoresDT = cross_val_score(dtree,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "print(\"Accuracy with Decision Tree: %0.5f (+/- %0.5f)\" % (scoresDT.mean(), scoresDT.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Gaussian Naive Bayes: 0.59660 (+/- 0.03648)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gauss = GaussianNB()\n",
    "scores = cross_val_score(gauss,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "print(\"Accuracy with Gaussian Naive Bayes: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with MLPC Classifier with 1 : 0.15709 (+/- 0.01881)\n",
      "Accuracy with MLPC Classifier with 0.1 : 0.16379 (+/- 0.01354)\n",
      "Accuracy with MLPC Classifier with 0.01 : 0.54439 (+/- 0.11737)\n",
      "Accuracy with MLPC Classifier with 0.001 : 0.53689 (+/- 0.07251)\n",
      "Accuracy with MLPC Classifier with 0.002 : 0.50753 (+/- 0.13606)\n",
      "Accuracy with MLPC Classifier with 0.003 : 0.50330 (+/- 0.10057)\n",
      "Accuracy with MLPC Classifier with 0.004 : 0.52016 (+/- 0.07763)\n",
      "Accuracy with MLPC Classifier with 0.005 : 0.50540 (+/- 0.09176)\n",
      "Accuracy with MLPC Classifier with 0.006 : 0.44530 (+/- 0.15944)\n",
      "Accuracy with MLPC Classifier with 0.0001 : 0.55458 (+/- 0.05086)\n",
      "Accuracy with MLPC Classifier with 1e-05 : 0.12513 (+/- 0.01994)\n",
      "Accuracy with MLPC Classifier with 1e-06 : 0.12680 (+/- 0.01770)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "L = [1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.006,0.0001,0.00001, 0.000001]\n",
    "for l in L: \n",
    "    MLPC = MLPClassifier(random_state=0, learning_rate='adaptive',max_iter=10000, learning_rate_init=l)\n",
    "    scoresMLPC = cross_val_score(MLPC,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "    print(\"Accuracy with MLPC Classifier with\",l,\": %0.5f (+/- %0.5f)\" % (scoresMLPC.mean(), scoresMLPC.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='5' and loss='logistic_regression' is not supported, Parameters: penalty=5, loss='logistic_regression', dual=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9780c0d1106d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.67\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0;32m    910\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[1;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[0;32m    770\u001b[0m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[0;32m    771\u001b[0m                      \u001b[1;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m                      % (error_string, penalty, loss, dual))\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='5' and loss='logistic_regression' is not supported, Parameters: penalty=5, loss='logistic_regression', dual=False"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(data,target,train_size=.67, random_state=0)\n",
    "clf = LogisticRegression(5, max_iter = 1000, random_state=0)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='5' and loss='logistic_regression' is not supported, Parameters: penalty=5, loss='logistic_regression', dual=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aa205a3fab42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#score = clf.decision_function(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0;32m    910\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[1;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[0;32m    770\u001b[0m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[0;32m    771\u001b[0m                      \u001b[1;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m                      % (error_string, penalty, loss, dual))\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='5' and loss='logistic_regression' is not supported, Parameters: penalty=5, loss='logistic_regression', dual=False"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(5, max_iter = 1000, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict(X_test)\n",
    "#score = clf.decision_function(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "a=accuracy_score(Y_test,predict)\n",
    "p=precision_score(Y_test,predict)\n",
    "r=recall_score(Y_test,predict)\n",
    "u = roc_auc_score(Y_test, score)\n",
    "print(\"%.3f\" % round(a,3))\n",
    "print(\"%.3f\" % round(p,3))\n",
    "print(\"%.3f\" % round(r,3))\n",
    "print(\"%.3f\" % round(u,3))\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Calculating accuracy of LOGISTIC REGRESSION using CROSS_VAL_SCORE\n",
    "C= [5,2,1,0.1,0.01,0.001,0.002,0.0001,0.00001]\n",
    "for c in C: \n",
    "    clf = LogisticRegression(C = c, max_iter = 1000, random_state=0, solver='sag')\n",
    "    scoresLR = cross_val_score(clf,X_train,y_train,cv=10,scoring='accuracy')\n",
    "    print(\"Accuracy with C = \",c,\": %0.5f(+/- %0.5f)\" %(scoresLR.mean(), scoresLR.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lasso = linear_model.Lasso()\n",
    "reg = linear_model.LinearRegression()\n",
    "array_lasso = np.array(cross_val_score(lasso, data, target,cv=10))\n",
    "#print(cross_val_score(reg, data,target,cv=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC()\n",
    "scores = cross_val_score(clf,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "print(\"Accuracy with Support Vector Machines: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(data,target,train_size=.67, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,predict)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sklearn.datasets.fetch_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "data_sk = fetch_covtype()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(data_sk.data,data_sk.target,train_size=.67, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65451174289245984"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.65451(+/- 0.00000)\n",
      "Precision Score : 0.64130(+/- 0.24404)\n",
      "Recall Score : 0.64639(+/- 0.39492)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "a=accuracy_score(Y_test,predict)\n",
    "p=precision_score(Y_test,predict,average=None)\n",
    "r=recall_score(Y_test,predict,average=None)\n",
    "print(\"Accuracy Score : %0.5f(+/- %0.5f)\" %(a.mean(), a.std()*2))\n",
    "print(\"Precision Score : %0.5f(+/- %0.5f)\" %(p.mean(), p.std()*2))\n",
    "print(\"Recall Score : %0.5f(+/- %0.5f)\" %(r.mean(), r.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654511742892 [ 0.62404092  0.57206208  0.48355263  0.77114428  0.61075949  0.56218905\n",
      "  0.86534216] [ 0.56481481  0.53862213  0.3559322   0.90643275  0.74517375  0.5\n",
      "  0.91375291]\n"
     ]
    }
   ],
   "source": [
    "print(a,p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
